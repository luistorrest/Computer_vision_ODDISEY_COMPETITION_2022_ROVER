{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Downloading libraries and packages"
      ],
      "metadata": {
        "id": "3Y2yCP_Dm2l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import random"
      ],
      "metadata": {
        "id": "J2W8VlegDeWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's mount Drive"
      ],
      "metadata": {
        "id": "1y6wd6Bvc0ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "1xnKWs3CasOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downloading Weights and model preparation"
      ],
      "metadata": {
        "id": "jytBGhKiPGaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone --depth 1 https://github.com/tensorflow/models\n",
        "wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
        "mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "metadata": {
        "id": "HOj2vdS-TXTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import colab_utils\n",
        "from object_detection.builders import model_builder"
      ],
      "metadata": {
        "id": "jWX_Q8F9TPGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define the model architecture"
      ],
      "metadata": {
        "id": "69eP5yJDPpkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes=2\n",
        "pipeline_config_path='/content/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/pipeline.config'\n",
        "checkpoint_path='/content/models/research/object_detection/test_data/checkpoint/ckpt-0'"
      ],
      "metadata": {
        "id": "cxmhVPP9PmTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to use a method from the object detection API, with this method we will get the configuration of the model from a Pipeline Path"
      ],
      "metadata": {
        "id": "kF5WMv66QJ4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configs=config_util.get_configs_from_pipeline_file(pipeline_config_path)"
      ],
      "metadata": {
        "id": "NKlt6sggQFQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will instantiate the model attribute from the config file to change some features as the number of classes to be predicted"
      ],
      "metadata": {
        "id": "usoVSaIqUcCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config=configs['model']\n",
        "model_config.ssd.num_classes=num_classes\n",
        "model_config.ssd.freeze_batchnorm=True"
      ],
      "metadata": {
        "id": "R1xE37_3Urtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to build our model. In order to do so, we have to use another method from the object detection API and set the training to True"
      ],
      "metadata": {
        "id": "UTkx4u6lVEgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "detection_model=model_builder.build(model_config=model_config, is_training=True)"
      ],
      "metadata": {
        "id": "EeQmUvYsVgZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RetinaNet model has two prediction heads, one for classification, and the another for box regression. We will initialize the classification head from scratch, and keep the bounding box head untouched"
      ],
      "metadata": {
        "id": "1z4CEa-CV11u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fake_box_predictor=tf.train.Checkpoint(_base_tower_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
        "                                       _box_prediction_head=detection_model._box_predictor._box_prediction_head)"
      ],
      "metadata": {
        "id": "T05bPZ_cVwm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to add the feature extractor to our model"
      ],
      "metadata": {
        "id": "-qtdxOPLXL2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fake_model=tf.train.Checkpoint(_feature_extractor=detection_model._feature_extractor, \n",
        "                               _box_predictor=fake_box_predictor)"
      ],
      "metadata": {
        "id": "4uIlQXOXXSld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load the saved weigths into to model"
      ],
      "metadata": {
        "id": "YyUseQT7XkP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt=tf.train.Checkpoint(model=fake_model)\n",
        "ckpt.restore(checkpoint_path).expect_partial()"
      ],
      "metadata": {
        "id": "KHCyEdX1XjJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, shapes=detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
        "prediction_dict=detection_model.predict(image, shapes)\n",
        "_=detection_model.postprocess(prediction_dict, shapes)"
      ],
      "metadata": {
        "id": "0Wv5mw8oYr9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Preparation"
      ],
      "metadata": {
        "id": "1m1ntQb9WVav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, we have to know how our model expects the data. We will prepare our model to receive the data with the next format:\n",
        "\n",
        "Image: A numpy array of a tensor with shape [640, 640, 3]\n",
        "\n",
        "Boxes: A numpy array of shape [N, 4]\n",
        "\n",
        "Classes: A numpy array with shape [N]. **Note that class indices must match the keys in the label map (we will talk about it later)**"
      ],
      "metadata": {
        "id": "L3hQystIg4DY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loading the image from a path contained in a json file"
      ],
      "metadata": {
        "id": "3WuZDO0MbnVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the last notebook, where we filtered and saved the data, we made a Json file that contained the Path of each image, now we are going to define a function to load the images from that path, let's wee how we can do it."
      ],
      "metadata": {
        "id": "61dH1dZlbvDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's download the json objects that contain the ids and bounding boxes of the images"
      ],
      "metadata": {
        "id": "QTRn48S-mhSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir /content/dataset_train\n",
        "mkdir /content/dataset_test\n",
        "\n",
        "cp -r /gdrive/MyDrive/Test /content/dataset_test\n",
        "cp -r /gdrive/MyDrive/Train /content/dataset_train\n",
        "unzip  /content/dataset_train/Train/train_images.zip\n",
        "unzip  /content/dataset_test/Test/test_images.zip "
      ],
      "metadata": {
        "id": "8SjAxzB4vZ5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/dataset_train/Train/train_bboxes.json') as json_file:\n",
        "    train_bboxes = json.load(json_file)\n",
        "    json_file.close()\n",
        "    train_bboxes=json.loads(train_bboxes)"
      ],
      "metadata": {
        "id": "qXKfnWabmwVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/dataset_train/Train/train_images_id.json') as json_file:\n",
        "    train_images_ids = json.load(json_file)\n",
        "    json_file.close()\n",
        "    train_images_ids=json.loads(train_images_ids)"
      ],
      "metadata": {
        "id": "PmnTNGBBr9Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/dataset_train/Train/train_classes.json') as json_file:\n",
        "    train_classes = json.load(json_file)\n",
        "    json_file.close()\n",
        "    train_classes=json.loads(train_classes)"
      ],
      "metadata": {
        "id": "2NX_Zg8G33C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This functions expects a json file that contains as a key the id of the image, and as its value the name of the image, its format is shown below:\n",
        "\n",
        "    'image_id': 'name of the image'\n",
        "\n",
        "We have to pass to the function the path of the folder that contains the images, and this function will parse that folder loading the images into a numpy array"
      ],
      "metadata": {
        "id": "r1DKig7_stHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_from_path(json_file, image_folder_path, key):\n",
        "\n",
        "  image_name=json_file[str(key)]\n",
        "\n",
        "  path=image_folder_path+image_name\n",
        "\n",
        "\n",
        "  image=cv2.imread(path, 1)\n",
        "  image=image[:, :, ::-1]\n",
        "\n",
        "  return image.astype(np.uint8)\n"
      ],
      "metadata": {
        "id": "sqWjmTRRcERm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following functions converts the numpy array images into tf tensors"
      ],
      "metadata": {
        "id": "LGmG9ObBldD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def images_to_tensors(json_file, image_folder_path):\n",
        "  images=[]\n",
        "  for i in json_file.keys():\n",
        "    image=load_image_from_path(json_file=json_file, image_folder_path=image_folder_path, key=i)\n",
        "    images.append(image)\n",
        "\n",
        "  return images"
      ],
      "metadata": {
        "id": "9EvHrh_lqlkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_image_to_tensor(json_file, image_folder_path, key):\n",
        "  image=[]\n",
        "  image_np=load_image_from_path(json_file=json_file, image_folder_path=image_folder_path, key=key)\n",
        "  image.append(image_np)\n",
        "  return image_np"
      ],
      "metadata": {
        "id": "5OQb7gF-faa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create another function to convert the list of bounding boxes into a list of tensors"
      ],
      "metadata": {
        "id": "ae-XNIP-liu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bboxes_to_tensors(dictionary):\n",
        "  boxes_tensor=[]\n",
        "  for i in dictionary.keys():\n",
        "    boxes_tensor.append(np.array(train_bboxes[i]).reshape(-1, 4))\n",
        "  return boxes_tensor"
      ],
      "metadata": {
        "id": "0nHZguo8anmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_bbox_to_tensor(json_file, key):\n",
        "  box_tensor=[]\n",
        "  boxes_list=(np.array(train_bboxes[key]).reshape(-1, 4))\n",
        "  box_tensor.append(boxes_list)\n",
        "  return box_tensor"
      ],
      "metadata": {
        "id": "vCL0RbOGdUPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the classes contained in another json file, so we have to create a new function to parse them"
      ],
      "metadata": {
        "id": "WiGc8F_khYBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classes_from_json(json_object):\n",
        "  classes_list=[]\n",
        "  for i in json_object.keys():\n",
        "    classes_list.append(np.array(json_object[i])+np.ones(np.array(json_object[i]).shape))\n",
        "  return classes_list"
      ],
      "metadata": {
        "id": "QSQJ6EX52n79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_one_class(json_file, key):\n",
        "  class_list=[]\n",
        "  class_np=np.array(json_file[key]+np.ones(np.array(json_file[key]).shape))\n",
        "  class_list.append(class_np)\n",
        "  return class_list"
      ],
      "metadata": {
        "id": "DqPl9Ffqdpbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the boxes, images and classes"
      ],
      "metadata": {
        "id": "6CwDNdmSmlMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boxes=bboxes_to_tensors(train_bboxes)"
      ],
      "metadata": {
        "id": "_ZjP2iPxmoXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images=images_to_tensors(train_images_ids, '/content/content/dataset_train_tensorflow/data/')"
      ],
      "metadata": {
        "id": "l2JEsQYqmvhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes=get_classes_from_json(train_classes)"
      ],
      "metadata": {
        "id": "ZHpOLVlSr-0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing the data"
      ],
      "metadata": {
        "id": "YzwCq6j8oQrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TensorFlow Object Detection API expects tensors as an Input, so we have to convert the classes, images and Bounding Boxes to Tensors.\n",
        "\n",
        "Remember the classes must be One Hot Enconded."
      ],
      "metadata": {
        "id": "e2AxD2JxgUX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before, we had a problem when training the Dataset, because we were loading all the data at once, instead of loading it with parts. We got a problem realeted with the RAM usage because TesnsorFlow was not capable to alocate all the vectors, so what we are going to do, is to define a function that allows us to charge the data when it is needed in order to solve the storage problems we had."
      ],
      "metadata": {
        "id": "EQ1px3D-SMQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def charge_data_batches(images, boxes, classes, tensor_indexes, num_classes):\n",
        "\n",
        "  # This list will contain the images as tensors\n",
        "  train_image_tensors=[]\n",
        "\n",
        "  # This list has inside it the Bounding Boxes. The DType of this boxes must be tf.float32\n",
        "  # And every tensor must be of shape [N_i, 4] where the N_i represents the bounding boxes that a single image contains\n",
        "  gt_box_tensors=[]\n",
        "\n",
        "  #The list of classes will have tensors with shape [N_i, Num_clases] where N_i will be the class associated wich each Bounding \n",
        "  # Box in the image, and Num_classes stands for the One Hot Encoding representation\n",
        "  gt_classes_tensors=[]\n",
        "\n",
        "  # The One Hot method from tensorflow expects a depth to create the tensors, that depth represents the number of classes \n",
        "  # of the dataset.\n",
        "  for index in tensor_indexes:\n",
        "\n",
        "    train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(images[index], dtype=tf.float32), axis=0))\n",
        "\n",
        "    gt_box_tensors.append(tf.convert_to_tensor(boxes[index], dtype=tf.float32))\n",
        "    #Tensorflow expects the classes One hot encoded, so we can use the method called one hot, where it \n",
        "    #returns the classes in the One hot ecoding format\n",
        "\n",
        "    gt_classes_tensors.append(tf.one_hot(classes[index], num_classes, dtype=tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # for (train_image_np, gt_box_np, gt_classes) in zip(images, boxes, classes):\n",
        "\n",
        "\n",
        "  return train_image_tensors, gt_box_tensors, gt_classes_tensors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #   train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(train_image_np, dtype=tf.float32), axis=0))\n",
        "\n",
        "  #   gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
        "\n",
        "  #   #Tensorflow expects the classes One hot encoded, so we can use the method called one hot, where it \n",
        "  #   #returns the classes in the One hot ecoding format\n",
        "  #   gt_classes_tensors.append(tf.one_hot(gt_classes, num_classes, dtype=tf.float32))\n",
        "\n"
      ],
      "metadata": {
        "id": "3lPm_pSLSyCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_class_id = 1\n",
        "car_class_id=2\n",
        "category_index = {car_class_id: {'id': car_class_id, 'name': 'Car'}, cat_class_id: {'id': cat_class_id, 'name': 'Cat'} }\n"
      ],
      "metadata": {
        "id": "bDX_wszhv4uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the model"
      ],
      "metadata": {
        "id": "xvtPTraj72eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we have to define the variables to be trained. As a normal neural network we can get the trainable variables by calling its attributes"
      ],
      "metadata": {
        "id": "0LaRZV8JZqM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_variables=detection_model.trainable_variables\n",
        "\n",
        "prefixes_to_train=['WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
        "  'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']"
      ],
      "metadata": {
        "id": "QWyMd2cMZ7k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_fine_tune=[]\n",
        "for var in trainable_variables:\n",
        "  if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
        "    to_fine_tune.append(var)"
      ],
      "metadata": {
        "id": "7WsKRuoiaryk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we have to define how our training loop will be:\n",
        "\n",
        "    #First we have to instantiate our model:\n",
        "    model=detection_model \n",
        "\n",
        "    #Then we define the number of epochs:\n",
        "    epochs=20\n",
        "\n",
        "    #Now we have to run through the training batches:\n",
        "\n",
        "    losses_train= train_data_for_one_epoch()\n",
        "\n",
        "    #We have to calculate the validation losses and metrics:\n",
        "\n",
        "    losses_val=perform_validation()\n",
        "\n",
        "    losses_train_mean= np.mean(losses_train)\n",
        "    losses_val_mean= np.mean(losses_val)"
      ],
      "metadata": {
        "id": "IjaBubMIKg4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we saw before, to run each step we have to divide the data in batches, which will reduce the amount of images stored in memory and also will help to speed up (in some cases) the training process."
      ],
      "metadata": {
        "id": "_JJno1PaNJuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's prepare the dataset with batches. We can get one single example of oour dataset by indexing it. So what we are going to do is to create a list with the indexes of the images, create a tf.Dataset with that and then batch that tf.Dataset"
      ],
      "metadata": {
        "id": "u07yUCcuNkWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexes=np.arange(0, len(images))"
      ],
      "metadata": {
        "id": "HkoS86UbL5GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now let's instantiate the Dataset Class \n",
        "Dataset=tf.data.Dataset.from_tensor_slices(indexes).shuffle(buffer_size=100)"
      ],
      "metadata": {
        "id": "aaDDdLCiOvNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noe let's create another variable to define the batch size, and the optimizer"
      ],
      "metadata": {
        "id": "tbyGrBZjQEBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=8\n",
        "optimizer=tf.keras.optimizers.Adam()"
      ],
      "metadata": {
        "id": "doNSZiF8L45f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset=Dataset.batch(batch_size=batch_size)"
      ],
      "metadata": {
        "id": "lDt52gwrL4xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the things we already have we can define a function to perform the training step for one epoch, and another function to calculat the loss of each batch"
      ],
      "metadata": {
        "id": "9C9-XS1rRF8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss_each_batch(model, vars_to_fine_tune, optimizer, bboxes, images, classes):\n",
        "  shapes=tf.constant(batch_size*[[641, 640, 3]], dtype=tf.int32)\n",
        "  \n",
        "  model.provide_groundtruth(groundtruth_boxes_list=bboxes, groundtruth_classes_list=classes)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    preprocessed_images=tf.concat([model.preprocess(image)[0] for image in images], axis=0)\n",
        "\n",
        "    prediction_dict=model.predict(preprocessed_images, shapes)\n",
        "    losses_dict=model.loss(prediction_dict, shapes)\n",
        "    total_loss=losses_dict['Loss/localization_loss']+losses_dict['Loss/classification_loss']\n",
        "    gradients=tape.gradient(total_loss, vars_to_fine_tune)\n",
        "    optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
        "  print('loss {}'.format(total_loss))\n",
        "\n",
        "  return total_loss"
      ],
      "metadata": {
        "id": "o3pnitkSUThk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_data_for_one_epoch():\n",
        "\n",
        "  for step, batch in enumerate(Dataset):\n",
        "    images_list, boxes_list, classes_list=charge_data_batches(images, boxes, classes, batch, 2)\n",
        "    print('batch {}'.format(step))\n",
        "\n",
        "    # Now we can compute the loss of this batch:\n",
        "    total_loss=calculate_loss_each_batch(detection_model, to_fine_tune, optimizer, boxes_list, images_list, classes_list)\n",
        "\n",
        "  return total_loss"
      ],
      "metadata": {
        "id": "WlfsggxeL4o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(10):\n",
        "#   train_data_for_one_epoch()"
      ],
      "metadata": {
        "id": "05ktFU_9Q11l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}