{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Instalar Librerias"
      ],
      "metadata": {
        "id": "fL0URqTYI6Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install fiftyone\n"
      ],
      "metadata": {
        "id": "0Q9ljqfywt6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "metadata": {
        "id": "tAIM7NtCSdoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install libraries!!\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "from shutil import copyfile\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "import zipfile\n",
        "import glob\n",
        "import fiftyone\n",
        "import fiftyone.zoo as foz\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "GQiKCGo8JnVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make a code easy to read, we are going to define a function to download the datasets:"
      ],
      "metadata": {
        "id": "YKPfk016cIst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset(classes, max_samples, split, seed=51):\n",
        "  dataset= fiftyone.zoo.load_zoo_dataset(\n",
        "    \"open-images-v6\",\n",
        "    label_types=[\"detections\", \"classifications\"],\n",
        "    classes=classes,\n",
        "    max_samples=max_samples,\n",
        "    seed=seed,\n",
        "    shuffle=True,\n",
        "    split=split,\n",
        "    label_field='ground_truth')\n",
        "  return dataset\n",
        "  "
      ],
      "metadata": {
        "id": "nDNBIvLAxQke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train= download_dataset(['Car', 'Cat'], 1000, 'train')"
      ],
      "metadata": {
        "id": "N8WEW87Mk1aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fiftyone.launch_app(dataset_train)"
      ],
      "metadata": {
        "id": "2BmRWJerUGvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test=download_dataset(['Car', 'Cat'], 250, split='validation')"
      ],
      "metadata": {
        "id": "Z5ui37yPyUFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to apply a filter to the background labels in order to reduce the number of classes, because fiftyone although we had set a limit when downloading the dataset, this library automatically repopulates them"
      ],
      "metadata": {
        "id": "dk6AhRhD9eK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = dataset_train.filter_labels(\"ground_truth_detections\", fiftyone.ViewField(\"label\").is_in([\"Cat\", \"Car\"]))"
      ],
      "metadata": {
        "id": "U1j4eMHJi5b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test = dataset_test.filter_labels(\"ground_truth_detections\", fiftyone.ViewField(\"label\").is_in([\"Cat\", \"Car\"]))"
      ],
      "metadata": {
        "id": "NBRZnHnItTnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's export the datasets with the **coco** format"
      ],
      "metadata": {
        "id": "3omR9qqfT50o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.export('/content/dataset_train_tensorflow', dataset_type=fiftyone.types.COCODetectionDataset, label_field='ground_truth_detections')"
      ],
      "metadata": {
        "id": "gh4G7m7MoBYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test.export('/content/dataset_test_tensorflow', dataset_type=fiftyone.types.COCODetectionDataset, label_field='ground_truth_detections')"
      ],
      "metadata": {
        "id": "E_Bzd-SkUBve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Now let's make a Zip to export the images contained in the dataset"
      ],
      "metadata": {
        "id": "qWqedCxDU09l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "zip -r /content/train_images.zip /content/dataset_train_tensorflow/data\n",
        "zip -r /content/test_images.zip /content/dataset_test_tensorflow/data"
      ],
      "metadata": {
        "id": "xewzyOKCULfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to reduce the complexity of the labels, we are going to parse the **json** objects to save the bounding boxes as a dictionary in order to pass that lists to Tensorflow whe training the model. Let's define a funtions that parses teh Json objects and returns the dictionaries"
      ],
      "metadata": {
        "id": "K4ZAGbIKUK9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dictionaries(json_path):\n",
        "  with open(json_path, 'r') as json_file:\n",
        "    data=json.load(json_file)\n",
        "    json_file.close()\n",
        "    annotations=data['annotations']\n",
        "    data_image=data['images']\n",
        "  return annotations, data_image "
      ],
      "metadata": {
        "id": "BCJQwiGDy_Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to parse the data for the train and validation split with the function that we have defined before"
      ],
      "metadata": {
        "id": "5M6iqX1Pzvee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_annotations, train_data_image = get_dictionaries('/content/dataset_train_tensorflow/labels.json')"
      ],
      "metadata": {
        "id": "IVhzNQzKz7Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_annotations, test_data_image = get_dictionaries('/content/dataset_test_tensorflow/labels.json')"
      ],
      "metadata": {
        "id": "zZ5w-3pZxVym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a Dictionary that contains the bounding boxes for each image, the general structure of this dictionray will be the following one:\n",
        "\n",
        "datos={\n",
        "\n",
        "    'image_id':[b_boxes]\n",
        "}"
      ],
      "metadata": {
        "id": "fjDkt5yQin74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model from the object detection API expects the bounding boxes to be normalized, so what we have to do is to parse the shape of each image and then storage it in a dictionary, and finally we will normalize the bounding boxes."
      ],
      "metadata": {
        "id": "iKSCJx0hqHNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bounding_boxes_normalized(dict_with_boxes, dict_with_image_shape):\n",
        "  dict_with_shapes={}\n",
        "  shapes=dict_with_image_shape\n",
        "  data_boxes={}\n",
        "  bboxes=dict_with_boxes\n",
        "\n",
        "  for i in range(len(shapes)):\n",
        "    if shapes[i]['id'] not in dict_with_shapes:\n",
        "      dict_with_shapes[ shapes[i]['id']]=[shapes[i]['width'], shapes[i]['height']]\n",
        "\n",
        "  for i in range(len(bboxes)):\n",
        "    if bboxes[i]['image_id'] not in data_boxes:\n",
        "      bx, by, bw, bh =bboxes[i]['bbox']\n",
        "      nx, ny =dict_with_shapes[bboxes[i]['image_id']]\n",
        "      data_boxes[bboxes[i]['image_id']]=[bx/nx, by/ny, bw/nx, bh/ny]\n",
        "    \n",
        "    else:\n",
        "      bx, by, bw, bh =bboxes[i]['bbox']\n",
        "      nx, ny =dict_with_shapes[bboxes[i]['image_id']]\n",
        "      data_boxes[bboxes[i]['image_id']].extend([bx/nx, by/ny, bw/nx, bh/ny])\n",
        "  \n",
        "  return data_boxes\n"
      ],
      "metadata": {
        "id": "dvKTe4C01BTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_bboxes=bounding_boxes_normalized(train_annotations, train_data_image )"
      ],
      "metadata": {
        "id": "0uS5G0Ge4JDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_bboxes=bounding_boxes_normalized(test_annotations, test_data_image )"
      ],
      "metadata": {
        "id": "1YiZ2zYK45C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create another json object that contains the class id in each image:\n",
        "\n",
        "This dictionary has the following format:\n",
        "\n",
        "    image_id:[classes contained in the image]"
      ],
      "metadata": {
        "id": "MRjNod6njEcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dict_with_classes(dictionary):\n",
        "  classes={}\n",
        "  dict_with_classes=dictionary\n",
        "  for i in range (len(dict_with_classes)):\n",
        "    if dict_with_classes[i]['image_id'] not in classes:\n",
        "      classes[dict_with_classes[i]['image_id']]=[dict_with_classes[i]['category_id']]\n",
        "    else:\n",
        "      classes[dict_with_classes[i]['image_id']].extend([dict_with_classes[i]['category_id']])\n",
        "  return classes"
      ],
      "metadata": {
        "id": "oayZXihL6xpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_classes=get_dict_with_classes(train_annotations)"
      ],
      "metadata": {
        "id": "xQ4jWffh7zbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_classes=get_dict_with_classes(test_annotations)"
      ],
      "metadata": {
        "id": "qSDo0yWy8G_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the boxes, we have the classes. The only thing we need is another dicitionary that contains as a **key** the id if the image, and as its **value** the name of the image. We need this dictionary to load the images from its path. We will do it in another notebook.\n",
        "The structure for this dictionary will be the next:\n",
        "\n",
        "\n",
        "      paths{\n",
        "        id:'name_of_the_image.jpg'\n",
        "      }"
      ],
      "metadata": {
        "id": "smzVK6SB8b3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's create another Json file that contains each classes contained in each image"
      ],
      "metadata": {
        "id": "c8SfDNhvn0RG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_path(dictionary_with_path):\n",
        "  paths={}\n",
        "  img_ids=dictionary_with_path\n",
        "  for i in range(len(img_ids)):\n",
        "    if img_ids[i]['id'] not in paths:\n",
        "      paths[img_ids[i]['id']]=img_ids[i]['file_name']\n",
        "  return paths"
      ],
      "metadata": {
        "id": "zspeDWch98Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_id=get_image_path(train_data_image)"
      ],
      "metadata": {
        "id": "DsSYuGEA-o-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images_id=get_image_path(test_data_image)"
      ],
      "metadata": {
        "id": "6FYqNjM_-1S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we have all the data stored in dictionaries, no we will export them and save them in Google Drive"
      ],
      "metadata": {
        "id": "mHB4o6ST-6Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dicts(abs_path, dictionary):\n",
        "  json_data=json.dumps(dictionary, indent=4)\n",
        "  with open(abs_path, 'w+') as json_file:\n",
        "    json.dump(json_data, json_file)\n"
      ],
      "metadata": {
        "id": "wo1iLfKg_FC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dicts('/content/train_images_id.json', train_images_id)\n",
        "save_dicts('/content/test_images_id.json', test_images_id)\n",
        "save_dicts('/content/train_bboxes.json', train_bboxes)\n",
        "save_dicts('/content/test_bboxes.json', test_bboxes)\n",
        "save_dicts('/content/train_classes.json', train_classes)\n",
        "save_dicts('/content/test_classes.json', test_classes)\n"
      ],
      "metadata": {
        "id": "aVtXTqlb_pdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /gdrive/MyDrive/Test\n",
        "!rm -r /gdrive/MyDrive/Train"
      ],
      "metadata": {
        "id": "KX0n9lgeBR5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JUzlygZbVAmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "We already have the json files an zip directories containing the data for the train dataset, now let's do the same in order to create the test files\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yTu_2HrWXuXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir /gdrive/MyDrive/Train\n",
        "cp /content/train*  /gdrive/MyDrive/Train\n",
        "mkdir /gdrive/MyDrive/Test\n",
        "cp /content/test*  /gdrive/MyDrive/Test\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0_xm_ovcY5jG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}